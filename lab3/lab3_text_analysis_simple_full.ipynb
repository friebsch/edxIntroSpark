{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab3 的缩减版\n",
    "\n",
    "只留下最后全部数据处理所需要的必要内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (part0)读入文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "DATAFILE_PATTERN = '^(.+),\"(.+)\",(.*),(.*),(.*)'\n",
    "\n",
    "def removeQuotes(s):\n",
    "    \"\"\" Remove quotation marks from an input string\n",
    "    Args:\n",
    "        s (str): input string that might have the quote \"\" characters\n",
    "    Returns:\n",
    "        str: a string without the quote characters\n",
    "    \"\"\"\n",
    "    return ''.join(i for i in s if i!='\"')\n",
    "\n",
    "\n",
    "def parseDatafileLine(datafileLine):\n",
    "    \"\"\" Parse a line of the data file using the specified regular expression pattern\n",
    "    Args:\n",
    "        datafileLine (str): input string that is a line from the data file\n",
    "    Returns:\n",
    "        str: a string parsed using the given regular expression and without the quote characters\n",
    "    \"\"\"\n",
    "    match = re.search(DATAFILE_PATTERN, datafileLine)\n",
    "    if match is None:\n",
    "        print 'Invalid datafile line: %s' % datafileLine\n",
    "        return (datafileLine, -1)\n",
    "    elif match.group(1) == '\"id\"':\n",
    "        print 'Header datafile line: %s' % datafileLine\n",
    "        return (datafileLine, 0)\n",
    "    else:\n",
    "        product = '%s %s %s' % (match.group(2), match.group(3), match.group(4))\n",
    "        return ((removeQuotes(match.group(1)), product), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google.csv - Read 3227 lines, successfully parsed 3226 lines, failed to parse 0 lines\n",
      "Amazon.csv - Read 1364 lines, successfully parsed 1363 lines, failed to parse 0 lines\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from test_helper import Test\n",
    "\n",
    "baseDir = os.path.join('data')\n",
    "inputPath = os.path.join('cs100', 'lab3')\n",
    "\n",
    "GOOGLE_PATH = 'Google.csv'\n",
    "GOOGLE_SMALL_PATH = 'Google_small.csv'\n",
    "AMAZON_PATH = 'Amazon.csv'\n",
    "AMAZON_SMALL_PATH = 'Amazon_small.csv'\n",
    "GOLD_STANDARD_PATH = 'Amazon_Google_perfectMapping.csv'\n",
    "STOPWORDS_PATH = 'stopwords.txt'\n",
    "\n",
    "def parseData(filename):\n",
    "    \"\"\" Parse a data file\n",
    "    Args:\n",
    "        filename (str): input file name of the data file\n",
    "    Returns:\n",
    "        RDD: a RDD of parsed lines\n",
    "    \"\"\"\n",
    "    return (sc\n",
    "            .textFile(filename, 4, 0)\n",
    "            .map(parseDatafileLine)\n",
    "            .cache())\n",
    "\n",
    "def loadData(path):\n",
    "    \"\"\" Load a data file\n",
    "    Args:\n",
    "        path (str): input file name of the data file\n",
    "    Returns:\n",
    "        RDD: a RDD of parsed valid lines\n",
    "    \"\"\"\n",
    "    filename = os.path.join(baseDir, inputPath, path)\n",
    "    raw = parseData(filename).cache()\n",
    "    failed = (raw\n",
    "              .filter(lambda s: s[1] == -1)\n",
    "              .map(lambda s: s[0]))\n",
    "    for line in failed.take(10):\n",
    "        print '%s - Invalid datafile line: %s' % (path, line)\n",
    "    valid = (raw\n",
    "             .filter(lambda s: s[1] == 1)\n",
    "             .map(lambda s: s[0])\n",
    "             .cache())\n",
    "    print '%s - Read %d lines, successfully parsed %d lines, failed to parse %d lines' % (path,\n",
    "                                                                                        raw.count(),\n",
    "                                                                                        valid.count(),\n",
    "                                                                                        failed.count())\n",
    "    assert failed.count() == 0\n",
    "    assert raw.count() == (valid.count() + 1)\n",
    "    return valid\n",
    "\n",
    "google = loadData(GOOGLE_PATH)\n",
    "amazon = loadData(AMAZON_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1a)Tokenize a String\n",
    "the function simpleTokenize(string) that takes a string and returns a list of non-empty tokens in the string. simpleTokenize should split strings using the provided regular expression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "quickbrownfox = 'A quick brown fox jumps over the lazy dog.'\n",
    "split_regex = r'\\W+'\n",
    "\n",
    "def simpleTokenize(string):\n",
    "    \"\"\" A simple implementation of input string tokenization\n",
    "    Args:\n",
    "        string (str): input string\n",
    "    Returns:\n",
    "        list: a list of tokens\n",
    "    \"\"\"\n",
    "    # return <FILL IN>\n",
    "    return [item for item in re.split(split_regex, string.lower(), flags=re.IGNORECASE) if item not in set([''])]\n",
    "    # return re.sub(split_regex, \" \", string).lower().strip().split() \n",
    "\n",
    "print simpleTokenize(quickbrownfox) # Should give ['a', 'quick', 'brown', ... ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1b)Removing stopwords\n",
    "\n",
    "Using the included file \"stopwords.txt\", implement tokenize, an improved tokenizer that does not emit stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the stopwords: set([u'all', u'just', u'being', u'over', u'both', u'through', u'yourselves', u'its', u'before', u'with', u'had', u'should', u'to', u'only', u'under', u'ours', u'has', u'do', u'them', u'his', u'very', u'they', u'not', u'during', u'now', u'him', u'nor', u'did', u'these', u't', u'each', u'where', u'because', u'doing', u'theirs', u'some', u'are', u'our', u'ourselves', u'out', u'what', u'for', u'below', u'does', u'above', u'between', u'she', u'be', u'we', u'after', u'here', u'hers', u'by', u'on', u'about', u'of', u'against', u's', u'or', u'own', u'into', u'yourself', u'down', u'your', u'from', u'her', u'whom', u'there', u'been', u'few', u'too', u'themselves', u'was', u'until', u'more', u'himself', u'that', u'but', u'off', u'herself', u'than', u'those', u'he', u'me', u'myself', u'this', u'up', u'will', u'while', u'can', u'were', u'my', u'and', u'then', u'is', u'in', u'am', u'it', u'an', u'as', u'itself', u'at', u'have', u'further', u'their', u'if', u'again', u'no', u'when', u'same', u'any', u'how', u'other', u'which', u'you', u'who', u'most', u'such', u'why', u'a', u'don', u'i', u'having', u'so', u'the', u'yours', u'once'])\n",
      "['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "stopfile = os.path.join(baseDir, inputPath, STOPWORDS_PATH)\n",
    "stopwords = set(sc.textFile(stopfile).collect())\n",
    "print 'These are the stopwords: %s' % stopwords\n",
    "\n",
    "def tokenize(string):\n",
    "    \"\"\" An implementation of input string tokenization that excludes stopwords\n",
    "    Args:\n",
    "        string (str): input string\n",
    "    Returns:\n",
    "        list: a list of tokens without stopwords\n",
    "    \"\"\"\n",
    "    # return <FILL IN>\n",
    "    return [item for item in simpleTokenize(string) if item not in stopwords]\n",
    "\n",
    "print tokenize(quickbrownfox) # Should give ['quick', 'brown', ... ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2a) Implement a TF function\n",
    "\n",
    "Implement tf(tokens) that takes a list of tokens and returns a Python dictionary mapping tokens to TF weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'brown': 0.16666666666666666, 'lazy': 0.16666666666666666, 'jumps': 0.16666666666666666, 'fox': 0.16666666666666666, 'dog': 0.16666666666666666, 'quick': 0.16666666666666666}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def tf(tokens):\n",
    "    \"\"\" Compute TF\n",
    "    Args:\n",
    "        tokens (list of str): input list of tokens from tokenize\n",
    "    Returns:\n",
    "        dictionary: a dictionary of tokens to its TF values\n",
    "    \"\"\"\n",
    "    # <FILL IN>\n",
    "    dict = {}\n",
    "    for t in tokens:\n",
    "        if dict.has_key(t):\n",
    "            dict[t] += 1.0;\n",
    "        else:\n",
    "            dict[t] = 1.0;\n",
    "    \n",
    "    count = len(tokens)\n",
    "    for key in dict.keys():\n",
    "        dict[key] /= count\n",
    "        \n",
    "    # return <FILL IN>\n",
    "    return dict\n",
    "\n",
    "print tf(tokenize(quickbrownfox)) # Should give { 'quick': 0.1666 ... }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2c) Implement an IDFs function\n",
    "\n",
    "compute the IDF weights for all tokens in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def idfs(corpus):\n",
    "    \"\"\" Compute IDF\n",
    "    Args:\n",
    "        corpus (RDD): input corpus\n",
    "    Returns:\n",
    "        RDD: a RDD of (token, IDF value)\n",
    "    \"\"\"\n",
    "    # N: the total number of documents in U\n",
    "    # N = <FILL IN>\n",
    "    N = corpus.count()\n",
    "    \n",
    "    # uniqueTokens = corpus.<FILL IN>\n",
    "    uniqueTokens = corpus.flatMap(lambda doc: tf(doc[1]).keys())\n",
    "    \n",
    "    # tokenCountPairTuple = uniqueTokens.<FILL IN>\n",
    "    tokenCountPairTuple = uniqueTokens.map(lambda t : (t, 1))\n",
    "    \n",
    "    # tokenSumPairTuple = tokenCountPairTuple.<FILL IN>\n",
    "    tokenSumPairTuple = tokenCountPairTuple.reduceByKey(lambda v1, v2: v1+v2)\n",
    "    \n",
    "    # return (tokenSumPairTuple.<FILL IN>)\n",
    "    return (tokenSumPairTuple.map(lambda (k, v) : (k, N*1.0/v) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2f) Implement a TF-IDF function\n",
    "\n",
    "implement a tfidf(tokens, idfs) function that takes a list of tokens from a document and a Python dictionary of IDF weights and returns a Python dictionary mapping individual tokens to total TF-IDF weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def tfidf(tokens, idfs):\n",
    "    \"\"\" Compute TF-IDF\n",
    "    Args:\n",
    "        tokens (list of str): input list of tokens from tokenize\n",
    "        idfs (dictionary): record to IDF value\n",
    "    Returns:\n",
    "        dictionary: a dictionary of records to TF-IDF values\n",
    "    \"\"\"\n",
    "    # tfs = <FILL IN>\n",
    "    tfs = tf(tokens)\n",
    "    \n",
    "    # tfIdfDict = <FILL IN>\n",
    "    tfIdfDict = {}\n",
    "    for k in tfs.keys():\n",
    "        tfIdfDict[k] = tfs[k] * idfs[k]\n",
    "                \n",
    "    return tfIdfDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  calculate the square root of the dot product of a dictionary and itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def norm(a):\n",
    "    \"\"\" Compute square root of the dot product\n",
    "    Args:\n",
    "        a (dictionary): a dictionary of record to value\n",
    "    Returns:\n",
    "        norm: a dictionary of tokens to its TF values\n",
    "    \"\"\"\n",
    "    # return <FILL IN>\n",
    "    return math.sqrt(sum(a[k] * a[k] for k in a.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4a)Tokenize the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon full dataset is 1363 products, Google full dataset is 3226 products\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# amazonFullRecToToken = amazon.<FILL IN>\n",
    "amazonFullRecToToken = amazon.map(lambda (k, v): (k, tokenize(v)))\n",
    "\n",
    "# googleFullRecToToken = google.<FILL IN>\n",
    "googleFullRecToToken = google.map(lambda (k, v): (k, tokenize(v)))\n",
    "\n",
    "print 'Amazon full dataset is %s products, Google full dataset is %s products' % (amazonFullRecToToken.count(),\n",
    "                                                                                    googleFullRecToToken.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4b)Compute IDFs and TF-IDFs for the full datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 17078 unique tokens in the full datasets.\n",
      "There are 1363 Amazon weights and 3226 Google weights.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# fullCorpusRDD = <FILL IN>\n",
    "fullCorpusRDD = amazonFullRecToToken.union(googleFullRecToToken)\n",
    "\n",
    "idfsFull = idfs(fullCorpusRDD)\n",
    "idfsFullCount = idfsFull.count()\n",
    "print 'There are %s unique tokens in the full datasets.' % idfsFullCount\n",
    "\n",
    "# Recompute IDFs for full dataset\n",
    "# idfsFullWeights = <FILL IN>\n",
    "idfsFullWeights = idfsFull.collectAsMap()\n",
    "\n",
    "# idfsFullBroadcast = <FILL IN>\n",
    "idfsFullBroadcast = sc.broadcast(idfsFullWeights)\n",
    "\n",
    "# Pre-compute TF-IDF weights.  Build mappings from record ID weight vector.\n",
    "# amazonWeightsRDD = <FILL IN>\n",
    "amazonWeightsRDD = amazonFullRecToToken.map(lambda (k,v): (k, tfidf(v, idfsFullBroadcast.value)))\n",
    "\n",
    "# googleWeightsRDD = <FILL IN>\n",
    "googleWeightsRDD = googleFullRecToToken.map(lambda (k,v): (k, tfidf(v, idfsFullBroadcast.value)))\n",
    "\n",
    "print 'There are %s Amazon weights and %s Google weights.' % (amazonWeightsRDD.count(),\n",
    "                                                              googleWeightsRDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4c)Compute Norms for the weights from the full datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# amazonNorms = amazonWeightsRDD.<FILL IN>\n",
    "amazonNorms = amazonWeightsRDD.map(lambda (k, v): (k, norm(v)))\n",
    "\n",
    "#amazonNormsBroadcast = <FILL IN>\n",
    "amazonNormsBroadcast = sc.broadcast(amazonNorms.collectAsMap())\n",
    "\n",
    "#googleNorms = googleWeightsRDD.<FILL IN>\n",
    "googleNorms = googleWeightsRDD.map(lambda (k, v): (k, norm(v)))\n",
    "\n",
    "#googleNormsBroadcast = <FILL IN>\n",
    "googleNormsBroadcast = sc.broadcast(googleNorms.collectAsMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4d)Create inverted indicies from the full datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 111387 Amazon inverted pairs and 77678 Google inverted pairs.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def invert(record):\n",
    "    \"\"\" Invert (ID, tokens) to a list of (token, ID)\n",
    "    Args:\n",
    "        record: a pair, (ID, token vector)\n",
    "    Returns:\n",
    "        pairs: a list of pairs of token to ID\n",
    "    \"\"\"\n",
    "    # <FILL IN>\n",
    "    id = record[0]\n",
    "    pairs = []\n",
    "    for item in record[1].keys():\n",
    "        pairs.append((item, id))\n",
    "    \n",
    "    return (pairs)\n",
    "\n",
    "amazonInvPairsRDD = (amazonWeightsRDD\n",
    "#                    .<FILL IN>\n",
    "                     .flatMap(invert)\n",
    "                    .cache())\n",
    "                    \n",
    "googleInvPairsRDD = (googleWeightsRDD\n",
    "#                    .<FILL IN>\n",
    "                    .flatMap(invert)\n",
    "                    .cache())\n",
    "\n",
    "print 'There are %s Amazon inverted pairs and %s Google inverted pairs.' % (amazonInvPairsRDD.count(),\n",
    "                                                                            googleInvPairsRDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4e)Identify common tokens from the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2441100 common tokens\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def swap(record):\n",
    "    \"\"\" Swap (token, (ID, URL)) to ((ID, URL), token)\n",
    "    Args:\n",
    "        record: a pair, (token, (ID, URL))\n",
    "    Returns:\n",
    "        pair: ((ID, URL), token)\n",
    "    \"\"\"\n",
    "    # token = <FILL IN>\n",
    "    token = record[0]\n",
    "    \n",
    "    # keys = <FILL IN>\n",
    "    keys = record[1]\n",
    "    \n",
    "    # used for reduceByKey\n",
    "    # return (keys, {token:1})\n",
    "    \n",
    "    # used for groupByKey\n",
    "    return (keys, token)\n",
    "\n",
    "commonTokens = (amazonInvPairsRDD\n",
    "#                .<FILL IN>\n",
    "                .join(googleInvPairsRDD)\n",
    "                .map(swap)\n",
    "                .groupByKey()\n",
    "#                .reduceByKey(lambda v1,v2: { k:(v1.get(k,0) + v2.get(k,0)) for k in set(v1.keys())|set(v2.keys()) })\n",
    "                .cache())\n",
    "# here groupByKey() and reduceByKey allmost cost same memory, but groupByKey() is simple and quick\n",
    "\n",
    "print 'Found %d common tokens' % commonTokens.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4f)Identify common tokens from the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# amazonWeightsBroadcast = <FILL IN>\n",
    "amazonWeightsBroadcast = sc.broadcast(amazonWeightsRDD.collectAsMap())\n",
    "\n",
    "#googleWeightsBroadcast = <FILL IN>\n",
    "googleWeightsBroadcast = sc.broadcast(googleWeightsRDD.collectAsMap())\n",
    "\n",
    "# for debug\n",
    "# longTokens = commonTokens.filter(lambda (k,v): len(v)>1).top(1)[0]\n",
    "# print longTokens\n",
    "\n",
    "# for item in longTokens[1]:\n",
    "#      print item\n",
    "\n",
    "def fastCosineSimilarity(record):\n",
    "    \"\"\" Compute Cosine Similarity using Broadcast variables\n",
    "    Args:\n",
    "        record: ((ID, URL), token)\n",
    "    Returns:\n",
    "        pair: ((ID, URL), cosine similarity value)\n",
    "    \"\"\"      \n",
    "    # amazonRec = <FILL IN>\n",
    "    amazonRec = record[0][0]\n",
    "    \n",
    "    # googleRec = <FILL IN>\n",
    "    googleRec = record[0][1]\n",
    "\n",
    "    # tokens = <FILL IN>\n",
    "    tokens = record[1]\n",
    "       \n",
    "    # s = <FILL IN>\n",
    "    # used when reduceByKey in upper (4e)\n",
    "    s = sum(amazonWeightsBroadcast.value[amazonRec][key] * googleWeightsBroadcast.value[googleRec][key] * tokens[key] for key in tokens.keys())\n",
    "    \n",
    "    # used when groupByKey in upper (4e)\n",
    "    s = sum(amazonWeightsBroadcast.value[amazonRec][item] * googleWeightsBroadcast.value[googleRec][item] for item in tokens)\n",
    "    \n",
    "    # value = <FILL IN>\n",
    "    value = s/(amazonNormsBroadcast.value[amazonRec] * googleNormsBroadcast.value[googleRec])\n",
    "\n",
    "    key = (amazonRec, googleRec)\n",
    "    return (key, value)\n",
    "\n",
    "# print fastCosineSimilarity(longTokens)\n",
    "\n",
    "similaritiesFullRDD = (commonTokens\n",
    "#                       .<FILL IN>\n",
    "                       .map(fastCosineSimilarity)\n",
    "                       .cache())\n",
    "\n",
    "# print similaritiesFullRDD.top(10)\n",
    "\n",
    "# record = (('b00005lzly', 'http://www.google.com/base/feeds/snippets/13823221823254120257'), {'includes': 1, 'data': 1, 'complete': 1, 'software': 1})\n",
    "# print fastCosineSimilarity(record)\n",
    "\n",
    "print similaritiesFullRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5a) Counting True Positives, False Positives, and False Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an RDD of ((Amazon ID, Google URL), similarity score)\n",
    "simsFullRDD = similaritiesFullRDD.map(lambda x: (\"%s %s\" % (x[0][0], x[0][1]), x[1]))\n",
    "assert (simsFullRDD.count() == 2441100)\n",
    "\n",
    "# Create an RDD of just the similarity scores\n",
    "simsFullValuesRDD = (simsFullRDD\n",
    "                     .map(lambda x: x[1])\n",
    "                     .cache())\n",
    "assert (simsFullValuesRDD.count() == 2441100)\n",
    "\n",
    "# Look up all similarity scores for true duplicates\n",
    "\n",
    "# This helper function will return the similarity score for records that are in the gold standard and the simsFullRDD (True positives), and will return 0 for records that are in the gold standard but not in simsFullRDD (False Negatives).\n",
    "def gs_value(record):\n",
    "    if (record[1][1] is None):\n",
    "        return 0\n",
    "    else:\n",
    "        return record[1][1]\n",
    "\n",
    "# Join the gold standard and simsFullRDD, and then extract the similarities scores using the helper function\n",
    "trueDupSimsRDD = (goldStandard\n",
    "                  .leftOuterJoin(simsFullRDD)\n",
    "                  .map(gs_value)\n",
    "                  .cache())\n",
    "print 'There are %s true duplicates.' % trueDupSimsRDD.count()\n",
    "assert(trueDupSimsRDD.count() == 1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.accumulators import AccumulatorParam\n",
    "class VectorAccumulatorParam(AccumulatorParam):\n",
    "    # Initialize the VectorAccumulator to 0\n",
    "    def zero(self, value):\n",
    "        return [0] * len(value)\n",
    "\n",
    "    # Add two VectorAccumulator variables\n",
    "    def addInPlace(self, val1, val2):\n",
    "        for i in xrange(len(val1)):\n",
    "            val1[i] += val2[i]\n",
    "        return val1\n",
    "\n",
    "# Return a list with entry x set to value and all other entries set to 0\n",
    "def set_bit(x, value, length):\n",
    "    bits = []\n",
    "    for y in xrange(length):\n",
    "        if (x == y):\n",
    "          bits.append(value)\n",
    "        else:\n",
    "          bits.append(0)\n",
    "    return bits\n",
    "\n",
    "# Pre-bin counts of false positives for different threshold ranges\n",
    "BINS = 101\n",
    "nthresholds = 100\n",
    "def bin(similarity):\n",
    "    return int(similarity * nthresholds)\n",
    "\n",
    "# fpCounts[i] = number of entries (possible false positives) where bin(similarity) == i\n",
    "zeros = [0] * BINS\n",
    "fpCounts = sc.accumulator(zeros, VectorAccumulatorParam())\n",
    "\n",
    "def add_element(score):\n",
    "    global fpCounts\n",
    "    b = bin(score)\n",
    "    fpCounts += set_bit(b, 1, BINS)\n",
    "\n",
    "simsFullValuesRDD.foreach(add_element)\n",
    "\n",
    "# Remove true positives from FP counts\n",
    "def sub_element(score):\n",
    "    global fpCounts\n",
    "    b = bin(score)\n",
    "    fpCounts += set_bit(b, -1, BINS)\n",
    "\n",
    "trueDupSimsRDD.foreach(sub_element)\n",
    "\n",
    "def falsepos(threshold):\n",
    "    fpList = fpCounts.value\n",
    "    return sum([fpList[b] for b in range(0, BINS) if float(b) / nthresholds >= threshold])\n",
    "\n",
    "def falseneg(threshold):\n",
    "    return trueDupSimsRDD.filter(lambda x: x < threshold).count()\n",
    "\n",
    "def truepos(threshold):\n",
    "    return trueDupSimsRDD.count() - falsenegDict[threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5b) Precision, Recall, and F-measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Precision = true-positives / (true-positives + false-positives)\n",
    "# Recall = true-positives / (true-positives + false-negatives)\n",
    "# F-measure = 2 x Recall x Precision / (Recall + Precision)\n",
    "\n",
    "def precision(threshold):\n",
    "    tp = trueposDict[threshold]\n",
    "    return float(tp) / (tp + falseposDict[threshold])\n",
    "\n",
    "def recall(threshold):\n",
    "    tp = trueposDict[threshold]\n",
    "    return float(tp) / (tp + falsenegDict[threshold])\n",
    "\n",
    "def fmeasure(threshold):\n",
    "    r = recall(threshold)\n",
    "    p = precision(threshold)\n",
    "    return 2 * r * p / (r + p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5c) Line Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thresholds = [float(n) / nthresholds for n in range(0, nthresholds)]\n",
    "falseposDict = dict([(t, falsepos(t)) for t in thresholds])\n",
    "falsenegDict = dict([(t, falseneg(t)) for t in thresholds])\n",
    "trueposDict = dict([(t, truepos(t)) for t in thresholds])\n",
    "\n",
    "precisions = [precision(t) for t in thresholds]\n",
    "recalls = [recall(t) for t in thresholds]\n",
    "fmeasures = [fmeasure(t) for t in thresholds]\n",
    "\n",
    "print precisions[0], fmeasures[0]\n",
    "assert (abs(precisions[0] - 0.000532546802671) < 0.0000001)\n",
    "assert (abs(fmeasures[0] - 0.00106452669505) < 0.0000001)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(thresholds, precisions)\n",
    "plt.plot(thresholds, recalls)\n",
    "plt.plot(thresholds, fmeasures)\n",
    "plt.legend(['Precision', 'Recall', 'F-measure'])\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
